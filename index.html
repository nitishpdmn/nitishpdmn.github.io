<!doctype html>

<html lang="en" xmlns:og="http://ogp.me/ns#">
<head>
    <!-- tells IE to use the highest standards mode available to it -->
    <!--[if IE]>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <![endif]-->
    <link rel="icon" type="image/png" href="favicons/16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="favicons/32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="favicons/48.png" sizes="48x48" />
    <link rel="canonical" href="https://nitish.me/" />
    <meta name="theme-color" content="#fafafa">
    <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css" type="text/css" />
    <link rel="stylesheet" href="academicons/css/academicons.min.css" type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Slabo+27px|Source+Sans+Pro:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="style.css" type="text/css" />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta charset="utf-8">

    <title>Nitish Padmanaban</title>
    <meta name="author" content="Nitish Padmanaban">
    <meta property="og:title" content="Nitish Padmanaban" />
    <meta property="og:type" content="profile" />
    <meta property="og:url" content="https://nitish.me/" />
    <meta property="og:image" content="https://nitish.me/site_ims/profile_405.jpg" />
    <meta name="description" property="og:description" content="I’m currently working on automatically refocusing reading glasses at Zinn Labs. My research is focused on optical and computational techniques for improving visual perception." />

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Person",
        "name": "Nitish Padmanaban",
        "givenName": "Nitish",
        "familyName": "Padmanaban",
        "image": "https://nitish.me/site_ims/profile_405.jpg",
        "jobTitle": "PhD, Electrical Engineering",
        "affiliation": [
            {
                "@type": "Corporation",
                "name": "Zinn Labs",
                "url": "https://zinnlabs.com/about/"
            },
            {
                "@type": "Organization",
                "name": "Stanford Computational Imaging Lab",
                "sameAs": [
                    "https://www.computationalimaging.org",
                    "https://www.facebook.com/Stanford-Computational-Imaging-Lab-400289963641103/",
                    "https://www.youtube.com/channel/UCrjWHhrkZnq4jwqvtrx-jvA/"
                ]
            }
        ],
        "alumniOf": [
            {
                "@type": "CollegeOrUniversity",
                "name": "University of California, Berkeley",
                "sameAs": "https://en.wikipedia.org/wiki/University_of_California,_Berkeley"
            },
            {
                "@type": "CollegeOrUniversity",
                "name": "Stanford University",
                "sameAs": "https://en.wikipedia.org/wiki/Stanford_University"
            }
        ],
        "url": "https://nitish.me/",
        "sameAs": [
            "https://www.linkedin.com/in/nitishp/",
            "https://scholar.google.com/citations?user=uiNjnfgAAAAJ&hl=en",
            "https://www.ted.com/speakers/nitish_padmanaban?language=en",
            "https://orcid.org/0000-0002-1643-7413"
        ]
    }
    </script>

    <script type="text/javascript" src="ga.js" async></script>
</head>

<body>
    <div class="about" role="complementary">
        <img alt="profile photo" src="site_ims/profile_lq.jpg" data-src="site_ims/profile_[[270,405,540,600,900,1200,1400]].jpg">
        <div id="info">
            <h1 class="name">Nitish Padmanaban</h1>
            <div class="job">PhD, Electrical Engineering</div>
            <div class="email">me [at] <span class="emailRemove">REMOVETHIS</span>nitish [dot] <span class="emailRemove">REMOVETHIS</span>me</div>
            <!--<div class="address"></div>-->
            <div class="external"><!--
                --><a href="https://scholar.google.com/citations?user=uiNjnfgAAAAJ&hl=en" title="Google Scholar" alt="Google Scholar" class="acicons ai-google-scholar gscholar"></a><!--
                --><a href="https://www.linkedin.com/in/nitishp/" title="LinkedIn" alt="LinkedIn" class="fa-linkedin linkedin"></a><!--
                --><a href="uploads/cv.pdf" title="CV" alt="CV" class="acicons ai-cv cv"></a><!--
                --><!--<a href="https://orcid.org/0000-0002-1643-7413" title="ORCID" alt="ORCID" class="acicons ai-orcid orcid"></a>--><!--
            --></div>
        </div>
        <div id="bio">
            <h2>About</h2>
            <p>
                I’m currently working on automatically refocusing reading glasses at <a href="https://zinnlabs.com/">Zinn Labs</a>.
            </p>
            <p>
                My research is focused on using optical and computational techniques for improving visual perception in near-eye optical systems. My work includes building and evaluating displays to alleviate the vergence&ndash;accommodation conflict in virtual and augmented reality systems, addressing presbyopia in both physical and virtual settings, and also visual&ndash;vestibular conflicts as they relate to motion sickness in VR.
            </p>
            <p>
                I recently graduated with a PhD from Stanford EE in 2020. While at Stanford, I was advised by <a href="https://stanford.edu/~gordonwz/">Prof. Gordon Wetzstein</a> as part of the <a href="https://www.computationalimaging.org/">Stanford Computational Imaging Lab</a>. Additionally, I was supported by an <a href="https://nsfgrfp.org/">NSF Graduate Research Fellowship</a>. I received a Master’s from Stanford EE in 2017, and a Bachelor’s in EECS from UC Berkeley in 2015. In my time at Berkeley, my coursework focused on signal processing, and I was an undergraduate researcher in the <a href="https://bisl.berkeley.edu/">Berkeley Imaging Systems Lab</a>.
            </p>
        </div>
    </div>

    <div class="main" role="main">
        <div class="navbar" role="navigation">
            <ul>
                <li><a href="#sec_publications">Publications</a></li>
                <li><a href="#sec_abstracts">Abstracts</a></li>
                <li><a href="#sec_talks">Presentations</a></li>
            </ul>
            <div title="Filters" class="filter-button fa-filter disabled"><div></div></div>
            <div class="filter-selects disabled">
                <div class="filter-float">
                    <div class="select-wrapper"><select id="filter_chooser" title="choose filter">
                        <option>Filter Type</option>
                        <optgroup label="&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;"></optgroup>
                    </select></div>
                    <div class="select-wrapper"><select disabled id="filter_unchosen" title="disabled until filter chosen">
                        <option>Any&nbsp;</option>
                    </select></div>
                </div>
            </div>
        </div>
        <div class="section" id="sec_publications">
            <h1 class="title">Publications</h1>

            <div class="publication" id="neural_holo" data-year="2020" data-journal="SIGGRAPH Asia">
                <div class="imgwrap"><img alt="neural_holo display image" src="pub_ims/lq/neural_holo.png" data-src="pub_ims/[[ratio]]/neural_holo.jpg" data-left="204"></div>
                <h2>Neural Holography with Camera-in-the-Loop Training</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/neuralholography/">project page</a>,
                    <a href="#">ACM</a>,
                    <a href="http://www.computationalimaging.org/wp-content/uploads/2020/08/NeuralHolography_SIGAsia2020.pdf">full text</a>)
                </div>
                <p>
                    Holographic displays promise unprecedented capabilities for direct-view displays as well as virtual and augmented reality applications. However, one of the biggest challenges for computer-generated holography (CGH) is the fundamental tradeoff between algorithm runtime and achieved image quality, which has prevented high-quality holographic image synthesis at fast speeds. Moreover, the image quality achieved by most holographic displays is low, due to the mismatch between the optical wave propagation of the display and its simulated model. Here, we develop an algorithmic CGH framework that achieves unprecedented image fidelity and real-time framerates. Our framework comprises several parts, including a novel camera-in-the-loop optimization strategy that allows us to either optimize a hologram directly or train an interpretable model of the optical wave propagation and a neural network architecture that represents the first CGH algorithm capable of generating full-color high-quality holographic images at 1080p resolution in real time.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Peng, Y., Choi, S., Padmanaban, N., & Wetzstein, G. (2020). Neural Holography with Camera-in-the-Loop Training. <i>ACM SIGGRAPH Asia (Transactions on Graphics), 39</i>(6), 1.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Peng:2020:Neural,
    title={Neural Holography with Camera-in-the-Loop Training},
    author={Peng, Yifan and Choi, Suyeon and Padmanaban, Nitish and Wetzstein, Gordon},
    journal={ACM Transactions on Graphics (SIGGRAPH Asia)},
    year={2020},
    volume={39},
    number={6},
    pages={1:1--1:14}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="occlusion" data-year="2020" data-journal="IEEE (VR/TVCG)">
                <div class="imgwrap"><img alt="occlusion display image" src="pub_ims/lq/occlusion.png" data-src="pub_ims/[[ratio]]/occlusion.jpg" data-left="0"></div>
                <h2>Factored Occlusion: Single Spatial Light Modulator Occlusion-Capable Optical See-Through Augmented Reality Display</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/factored-occlusion-ar-display-ieee-vr-2020/">project page</a>,
                    <a href="https://ieeexplore.ieee.org/abstract/document/8998139/">IEEE Xplore</a>,
                    <a href="https://www.computationalimaging.org/wp-content/uploads/2020/02/ieee-vr_factored-occlusion.pdf">full text</a>)
                </div>
                <p>
                    Occlusion is a powerful visual cue that is crucial for depth perception and realism in optical see-through augmented reality (OST-AR). However, existing OST-AR systems additively overlay physical and digital content with beam combiners&hairsp;&mdash;&hairsp;an approach that does not easily support mutual occlusion, resulting in virtual objects that appear semi-transparent and unrealistic. In this work, we propose a new type of occlusion-capable OST-AR system. Rather than additively combining the real and virtual worlds, we employ a single digital micromirror device (DMD) to merge the respective light paths in a multiplicative manner. This unique approach allows us to simultaneously block light incident from the physical scene on a pixel-by-pixel basis while also modulating the light emitted by a light-emitting diode (LED) to display digital content. Our technique builds on mixed binary/continuous factorization algorithms to optimize time-multiplexed binary DMD patterns and their corresponding LED colors to approximate a target augmented reality (AR) scene. In simulations and with a prototype benchtop display, we demonstrate hard-edge occlusions, plausible shadows, and also gaze-contingent optimization of this novel display mode, which only requires a single spatial light modulator.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Krajancich, B.*, Padmanaban, N.*, & Wetzstein, G. (2020). Factored Occlusion: Single Spatial Light Modulator Occlusion-Capable Optical See-Through Augmented Reality Display. <i>IEEE Transactions on Visualization and Computer Graphics</i>.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Krajancich:2020:Occlusion,
    title={Factored Occlusion: Single Spatial Light Modulator Occlusion-Capable Optical See-Through Augmented Reality Display},
    author={Krajancich, Brooke and Padmanaban, Nitish and Wetzstein, Gordon},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    year={2020},
    volume={26},
    number={5},
    pages={1871--1879}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="olas" data-year="2019" data-journal="SIGGRAPH Asia">
                <div class="imgwrap"><img alt="olas display image" src="pub_ims/lq/olas.png" data-src="pub_ims/[[ratio]]/olas.jpg" data-left="90"></div>
                <h2>Holographic Near-Eye Displays Based on Overlap-Add Stereograms</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/holographic-near-eye-displays-based-on-overlap-add-stereograms-siggraph-asia-2019/">project page</a>,
                    <a href="https://dl.acm.org/citation.cfm?id=3356517">ACM</a>,
                    <a href="https://www.computationalimaging.org/wp-content/uploads/2019/08/olas2019.pdf">full text</a>)
                </div>
                <p>
                    Holographic near-eye displays are a key enabling technology for virtual and augmented reality (VR/AR) applications. Holographic stereograms (HS) are a method of encoding a light field into a hologram, which enables them to natively support view-dependent lighting effects. However, existing HS algorithms require the choice of a hogel size, forcing a tradeoff between spatial and angular resolution. Based on the fact that the short-time Fourier transform (STFT) connects a hologram to its observable light field, we develop the overlap-add stereogram (OLAS) as the correct method of &ldquo;inverting&rdquo; the light field into a hologram via the STFT. The OLAS makes more efficient use of the information contained within the light field than previous HS algorithms, exhibiting better image quality at a range of distances and hogel sizes. Most remarkably, the OLAS does not degrade spatial resolution with increasing hogel size, overcoming the spatio-angular resolution tradeoff that previous HS algorithms face. Importantly, the optimal hogel size of previous methods typically varies with the depth of every object in a scene, making the OLAS not only a hogel size&ndash;invariant method, but also nearly scene independent. We demonstrate the performance of the OLAS both in simulation and on a prototype near-eye display system, showing focusing capabilities and view-dependent effects.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Peng, Y., & Wetzstein, G. (2019). Holographic Near-Eye Displays Based on Overlap-Add Stereograms. <i>ACM SIGGRAPH Asia (Transactions on Graphics), 38</i>(6), 214.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Padmanaban:2019:OLAS,
    title={Holographic Near-Eye Displays Based on Overlap-Add Stereograms},
    author={Padmanaban, Nitish and Peng, Yifan and Wetzstein, Gordon},
    journal={ACM Transactions on Graphics (SIGGRAPH Asia)},
    year={2019},
    volume={38},
    number={6},
    pages={214:1--214:13}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="autofocals" data-year="2019" data-journal="Science Advances">
                <div class="imgwrap"><img alt="autofocals display image" src="pub_ims/lq/autofocals.png" data-src="pub_ims/[[ratio]]/autofocals.jpg" data-left="173"></div>
                <h2>Autofocals: Evaluating Gaze-Contingent Eyeglasses for Presbyopes</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/autofocals-evaluating-gaze-contingent-eyeglasses-for-presbyopes/">project page</a>,
                    <a href="https://advances.sciencemag.org/content/5/6/eaav6187">Science Advances</a>,
                    <a href="https://advances.sciencemag.org/content/advances/5/6/eaav6187.full.pdf">full text</a>)
                </div>
                <p>
                    As humans age, they gradually lose the ability to accommodate, or refocus, to near distances because of the stiffening of the crystalline lens. This condition, known as presbyopia, affects nearly 20% of people worldwide. We design and build a new presbyopia correction, autofocals, to externally mimic the natural accommodation response, combining eye tracker and depth sensor data to automatically drive focus-tunable lenses. We evaluated 19 users on visual acuity, contrast sensitivity, and a refocusing task. Autofocals exhibit better visual acuity when compared to monovision and progressive lenses while maintaining similar contrast sensitivity. On the refocusing task, autofocals are faster and, compared to progressives, also significantly more accurate. In a separate study, a majority of 23 of 37 users ranked autofocals as the best correction in terms of ease of refocusing. Our work demonstrates the superiority of autofocals over current forms of presbyopia correction and could affect the lives of millions.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Konrad, R., & Wetzstein, G. (2019). Autofocals: Evaluating Gaze-Contingent Eyeglasses for Presbyopes. <i>Science Advances, 5</i>(6), eaav6187.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Padmanaban:2019:Autofocals,
    title={Autofocals: Evaluating Gaze-Contingent Eyeglasses for Presbyopes},
    author={Padmanaban, Nitish and Konrad, Robert and Wetzstein, Gordon},
    journal={Science Advances},
    year={2019},
    volume={5},
    number={6},
    pages={eaav6187}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="ml_sickness" data-year="2018" data-journal="IEEE (VR/TVCG)">
                <div class="imgwrap"><img alt="ml_sickness display image" src="pub_ims/lq/ml_sickness.png" data-src="pub_ims/[[ratio]]/ml_sickness.jpg" data-left="122"></div>
                <h2>Towards a Machine-Learning Approach for Sickness Prediction in 360&deg; Stereoscopic Videos</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/towards-a-machine-learning-approach-for-sickness-prediction-in-360-stereoscopic-videos-ieee-vr-2018/">project page</a>,
                    <a href="https://ieeexplore.ieee.org/abstract/document/8267239/">IEEE Xplore</a>,
                    <a href="https://www.computationalimaging.org/wp-content/uploads/2018/02/ieee-vr_motion-sickness.pdf">full text</a>)
                </div>
                <p>
                    Virtual reality systems are widely believed to be the next major computing platform. There are, however, some barriers to adoption that must be addressed, such as that of motion sickness&hairsp;&mdash;&hairsp;which can lead to undesirable symptoms including postural instability, headaches, and nausea. Motion sickness in virtual reality occurs as a result of moving visual stimuli that cause users to perceive self-motion while they remain stationary in the real world. There are several contributing factors to both this perception of motion and the subsequent onset of sickness, including field of view, motion velocity, and stimulus depth. We verify first that differences in vection due to relative stimulus depth remain correlated with sickness. Then, we build a dataset of stereoscopic 3D videos and their corresponding sickness ratings in order to quantify their nauseogenicity, which we make available for future use. Using this dataset, we train a machine learning algorithm on hand-crafted features (quantifying speed, direction, and depth as functions of time) from each video, learning the contributions of these various features to the sickness ratings. Our predictor generally outperforms a na&iuml;ve estimate, but is ultimately limited by the size of the dataset. However, our result is promising and opens the door to future work with more extensive datasets. This and further advances in this space have the potential to alleviate developer and end user concerns about motion sickness in the increasingly commonplace virtual world.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N.*, Ruban, T.*, Sitzmann, V., Norcia, A. M., & Wetzstein, G. (2018). Towards a Machine-Learning Approach for Sickness Prediction in 360&deg; Stereoscopic Videos. <i>IEEE Transactions on Visualization and Computer Graphics, 24</i>(4), 1594&ndash;1603.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Padmanaban:2018:Sickness,
    title={Towards a Machine-Learning Approach for Sickness Prediction in 360$^\circ$ Stereoscopic Videos},
    author={Padmanaban, Nitish and Ruban, Timon and Sitzmann, Vincent and Norcia, Anthony M and Wetzstein, Gordon},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    year={2018},
    volume={24},
    number={4},
    pages={1594--1603}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="acc_inv" data-year="2017" data-journal="SIGGRAPH (Papers)">
                <div class="imgwrap"><img alt="acc_inv display image" src="pub_ims/lq/acc_inv.png" data-src="pub_ims/[[ratio]]/acc_inv.jpg" data-left="268"></div>
                <h2>Accommodation-Invariant Computational Near-Eye Displays</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/accommodation-invariant-near-eye-displays-siggraph-2017/">project page</a>,
                    <a href="https://dl.acm.org/citation.cfm?id=3073594">ACM</a>,
                    <a href="https://www.computationalimaging.org/wp-content/uploads/2017/07/AIDisplays-SIGGRAPH2017.pdf">full text</a>)
                </div>
                <p>
                    Although emerging virtual and augmented reality (VR/AR) systems can produce highly immersive experiences, they can also cause visual discomfort, eyestrain, and nausea. One of the sources of these symptoms is a mismatch between vergence and focus cues. In current VR/AR near-eye displays, a stereoscopic image pair drives the vergence state of the human visual system to arbitrary distances, but the accommodation, or focus, state of the eyes is optically driven towards a fixed distance. In this work, we introduce a new display technology, dubbed accommodation-invariant (AI) near-eye displays, to improve the consistency of depth cues in near-eye displays. Rather than producing correct focus cues, AI displays are optically engineered to produce visual stimuli that are invariant to the accommodation state of the eye. The accommodation system can then be driven by stereoscopic cues, and the mismatch between vergence and accommodation state of the eyes is significantly reduced. We validate the principle of operation of AI displays using a prototype display that allows for the accommodation state of users to be measured while they view visual stimuli using multiple different display modes.

                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Konrad, R., Padmanaban, N., Molner, K., Cooper, E. A., & Wetzstein, G. (2017). Accommodation-Invariant Computational Near-Eye Displays. <i>ACM SIGGRAPH (Transactions on Graphics), 36</i>(4), 88.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Konrad:2017:Accommodation,
    title={Accommodation-Invariant Computational Near-Eye Displays},
    author={Konrad, Robert and Padmanaban, Nitish and Molner, Keenan and Cooper, Emily A and Wetzstein, Gordon},
    journal={ACM Transactions on Graphics (TOG)},
    volume={36},
    number={4},
    pages={88:1--88:12},
    year={2017}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="gaze_cont" data-year="2017" data-journal="PNAS">
                <div class="imgwrap"><img alt="gaze_cont display image" src="pub_ims/lq/gaze.png" data-src="pub_ims/[[ratio]]/gaze.jpg" data-left="198"></div>
                <h2>Optimizing Virtual Reality for All Users through Gaze-Contingent and Adaptive Focus Displays</h2>
                <div class="links">
                    (<a href="https://www.computationalimaging.org/publications/optimizing-vr-with-gaze-contingent-and-adaptive-focus-displays/">project page</a>,
                    <a href="https://www.pnas.org/content/114/9/2183.abstract">PNAS</a>,
                    <a href="https://www.pnas.org/content/114/9/2183.full.pdf">full text</a>)
                </div>
                <p>
                    From the desktop to the laptop to the mobile device, personal computing platforms evolve over time. Moving forward, wearable computing is widely expected to be integral to consumer electronics and beyond. The primary interface between a wearable computer and a user is often a near-eye display. However, current generation near-eye displays suffer from multiple limitations: they are unable to provide fully natural visual cues and comfortable viewing experiences for all users. At their core, many of the issues with near-eye displays are caused by limitations in conventional optics. Current displays cannot reproduce the changes in focus that accompany natural vision, and they cannot support users with uncorrected refractive errors. With two prototype near-eye displays, we show how these issues can be overcome using display modes that adapt to the user via computational optics. By using focus-tunable lenses, mechanically actuated displays, and mobile gaze-tracking technology, these displays can be tailored to correct common refractive errors and provide natural focus cues by dynamically updating the system based on where a user looks in a virtual scene. Indeed, the opportunities afforded by recent advances in computational optics open up the possibility of creating a computing platform in which some users may experience better quality vision in the virtual world than in the real one.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Konrad, R., Stramer, T., Cooper, E. A., & Wetzstein, G. (2017). Optimizing Virtual Reality for All Users through Gaze-Contingent and Adaptive Focus Displays. <i>Proceedings of the National Academy of Sciences of the United States of America, 114</i>(9), 2183&ndash;2188.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@article{Padmanaban:2017:Optimizing,
    title={Optimizing Virtual Reality for All Users through Gaze-Contingent and Adaptive Focus Displays},
    author={Padmanaban, Nitish and Konrad, Robert and Stramer, Tal and Cooper, Emily A and Wetzstein, Gordon},
    journal={Proceedings of the National Academy of Sciences},
    year={2017},
    volume={114}, 
    number={9}, 
    pages={2183--2188}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>
        </div>

<!--Start abstracts here-->
        <div class="section" id="sec_abstracts">
            <h1 class="title">Abstracts</h1>

            <div class="publication" id="etech_neural_holo" data-year="2020" data-journal="SIGGRAPH (E-Tech)">
                <div class="imgwrap"><img alt="etech_neural_holo display image" src="pub_ims/lq/etech_neural_holo.png" data-src="pub_ims/[[ratio]]/etech_neural_holo.jpg" data-left="0"></div>
                <h2>Neural Holography</h2>
                <div class="links">
                    (<a href="https://dl.acm.org/doi/10.1145/3388534.3407295">ACM</a>, <a href="#neural_holo">full publication</a>)
                </div>
                <p>
                    Holographic displays promise unprecedented capabilities for direct-view displays as well as virtual and augmented reality (VR/AR) applications. However, one of the biggest challenges for computer-generated holography (CGH) is the fundamental tradeoff between algorithm runtime and achieved image quality, which has prevented high-quality holographic image synthesis at fast speeds. Moreover, the image quality achieved by most holographic displays is low, due to the mismatch between physical light transport of the display and its simulated model. Here, we develop an algorithmic CGH framework that achieves unprecedented image fidelity and real-time framerates. Our framework comprises several parts, including a novel camera-in-the-loop optimization strategy that allows us to either optimize a hologram directly or train an interpretable model of the physical light transport and a neural network architecture that represents the first CGH algorithm capable of generating full-color holographic images at 1080p resolution in real time.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Peng, Y., Choi, S., Padmanaban, N., Kim, J., & Wetzstein, G. (2020, August). Neural Holography. In <i>ACM SIGGRAPH 2020 Emerging Technologies</i> (p. 8). ACM.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@inproceedings{Peng:2020:NeuralETech,
    title={Neural Holography},
    author={Peng, Yifan and Choi, Suyeon and Padmanaban, Nitish and Kim, Jonghyun and Wetzstein, Gordon},
    booktitle={ACM SIGGRAPH 2020 Emerging Technologies},
    year={2020},
    month={August},
    pages={8:1--8:2},
    organization={ACM}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="etech_autofocals" data-year="2018" data-journal="SIGGRAPH (E-Tech)">
                <div class="imgwrap"><img alt="etech_autofocals display image" src="pub_ims/lq/etech_autofocals.png" data-src="pub_ims/[[ratio]]/etech_autofocals.jpg" data-left="70"></div>
                <h2>Autofocals: Gaze-Contingent Eyeglasses for Presbyopes</h2>
                <div class="links">
                    (<a href="https://dl.acm.org/citation.cfm?id=3214918">ACM</a>, <a href="#autofocals">full publication</a>)
                </div>
                <p>
                    Presbyopia, the loss of accommodation due to the stiffening of the crystalline lens, affects nearly 20% of the population worldwide. Traditional forms of presbyopia correction use fixed focal elements that inherently trade off field of view or stereo vision for a greater range of distances at which the wearer can see clearly. However, none of these offer the same natural refocusing enjoyed in youth. In this work, we built a new presbyopia correction, dubbed Autofocals, which externally mimics the natural accommodation response by combining data from eye trackers and a depth sensor, and then automatically drives focus-tunable lenses. In our testing, wearers generally reported that the Autofocals compare favorably with their own current corrective eyewear.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Konrad, R., & Wetzstein, G. (2018, August). Autofocals: Gaze-Contingent Eyeglasses for Presbyopes. In <i>ACM SIGGRAPH 2018 Emerging Technologies</i> (p. 3). ACM.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@inproceedings{Padmanaban:2018:Autofocals,
    title={Computational Focus-Tunable Near-Eye Displays},
    author={Padmanaban, Nitish and Konrad, Robert and Wetzstein, Gordon},
    booktitle={ACM SIGGRAPH 2018 Emerging Technologies},
    year={2018},
    month={August},
    pages={3:1--3:2},
    organization={ACM}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="monovision" data-year="2017" data-journal="OSA (3D)">
                <div class="imgwrap"><img alt="monovision display image" src="pub_ims/lq/monovision.png" data-src="pub_ims/[[ratio]]/monovision.jpg" data-left="245"></div>
                <h2>Evaluation of Accommodation Responses to Monovision for Virtual Reality</h2>
                <div class="links">
                    (<a href="https://www.osapublishing.org/abstract.cfm?uri=3D-2017-DM2F.3">OSA</a>,
                    <span class="copyright">
                        <a href="uploads/monovision2017.pdf" rel="nofollow">full text</a><!-- get rid of whitespace in text
                        --><span>&copy; 2017 Optical Society of America. One print or electronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modifications of the content of this paper are prohibited.</span><!-- get rid of whitespace in text
                    --></span>)
                </div>
                <p>
                    Emerging virtual and augmented reality (VR/AR) systems can produce highly immersive experiences, but also induce visual discomfort, eyestrain, and nausea for some users. One of the sources of these symptoms is the lack of natural focus cues in all current VR/AR near-eye displays. These displays project stereoscopic image pairs, driving the vergence state of the eyes to arbitrary distances. However, the accommodation, or focus state of the eyes, is optically driven to a fixed distance. In this work, we empirically evalaute monovision: a simple, yet unconventional method for potentially driving the accommodation state of the eyes to two distances by allowing each eye to drive focus to a different distance.  
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Konrad, R., & Wetzstein, G. (2017, June). Evaluation of Accommodation Response to Monovision for Virtual Reality. In <i>3D Image Acquisition and Display: Technology, Perception and Applications</i> (pp. DM2F.3). Optical Society of America.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@inproceedings{Padmanaban:2017:Evaluation,
    title={Evaluation of Accommodation Response to Monovision for Virtual Reality},
    author={Padmanaban, Nitish and Konrad, Robert and Wetzstein, Gordon},
    booktitle={3D Image Acquisition and Display: Technology, Perception and Applications},
    year={2017},
    month={June},
    pages={DM2F.3},
    organization={Optical Society of America}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="etech_focus" data-year="2016" data-journal="SIGGRAPH (E-Tech)">
                <div class="imgwrap"><img alt="etech_focus display image" src="pub_ims/lq/etech.png" data-src="pub_ims/[[ratio]]/etech.jpg" data-left="25"></div>
                <h2>Computational Focus-Tunable Near-Eye Displays</h2>
                <div class="links">
                    (<a href="https://dl.acm.org/citation.cfm?id=2929470">ACM</a>)
                </div>
                <p>
                    Immersive virtual and augmented reality systems (VR/AR) are entering the consumer market and have the potential to profoundly impact our society. Applications of these systems range from communication, entertainment, education, collaborative work, simulation and training to telesurgery, phobia treatment, and basic vision research. In every immersive experience, the primary interface between the user and the digital world is the near-eye display. Thus, developing near-eye display systems that provide a high-quality user experience is of the utmost importance. Many characteristics of near-eye displays that define the quality of an experience, such as resolution, refresh rate, contrast, and field of view, have been significantly improved in recent years. However, a significant source of visual discomfort prevails: the vergence-accommodation conflict (VAC). This visual conflict results from the fact that vergence cues, but not focus cues, are simulated in near-eye display systems. Indeed, natural focus cues are not supported by any existing near-eye display. Afforded by focus-tunable optics, we explore unprecedented display modes that tackle this issue in multiple ways with the goal of increasing visual comfort and providing more realistic visual experiences.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Konrad, R., Padmanaban, N., Cooper, E., & Wetzstein, G. (2016, July). Computational Focus-Tunable Near-Eye Displays. In <i>ACM SIGGRAPH 2016 Emerging Technologies</i> (p. 3). ACM.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@inproceedings{Konrad:2016:Computational,
    title={Computational Focus-Tunable Near-Eye Displays},
    author={Konrad, Robert and Padmanaban, Nitish and Cooper, Emily A and Wetzstein, Gordon},
    booktitle={ACM SIGGRAPH 2016 Emerging Technologies},
    year={2016},
    month={July},
    pages={3:1--3:2},
    organization={ACM}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>

            <div class="publication" id="MPI" data-year="2015" data-journal="IWMPI">
                <div class="imgwrap"><img alt="MPI display image" src="pub_ims/lq/mpi.png" data-src="pub_ims/[[ratio]]/mpi.jpg" data-left="400"></div>
                <h2>Active Feedback Real Time MPI Control Software</h2>
                <div class="links">
                    (<a href="https://ieeexplore.ieee.org/abstract/document/7107079/">IEEE Xplore</a>)
                </div>
                <p>
                    Real-time MPI has the potential to serve as a noninvasive alternative to X-ray angiography. In order to achieve real time MPI, we must (a.) generate vector-drive field waveforms at a location governed in real time by the physician, (b.) acquire the MPI image data in real time, (c.) reconstruct the MPI images in real time. We have designed our MPI data acquisition and control (DAQ) to enable all these steps in real time.
                </p>
                <!--googleoff: snippet-->
                <div class="cite">
                    <h3>Citation</h3>
                    <p>
                        Padmanaban, N., Orendorff, R. D., Konkle, J. J., Goodwill, P. W., & Conolly, S. M. (2015, March). Active Feedback Real Time MPI Control Software. In <i>Magnetic Particle Imaging (IWMPI), 2015 5th International Workshop on</i> (pp. 1&ndash;1). IEEE.
                    </p>
                </div>
                <div class="bibtex">
                    <h3>BibTeX</h3>
                    <pre>
@inproceedings{Padmanaban:2015:Active,
    title={Active Feedback Real Time MPI Control Software},
    author={Padmanaban, Nitish and Orendorff, Ryan D and Konkle, Justin J and Goodwill, Patrick W and Conolly, Steven M},
    booktitle={Magnetic Particle Imaging (IWMPI), 2015 5th International Workshop on},
    year={2015},
    month={March},
    pages={1--1},
    organization={IEEE}
}
                    </pre>
                </div>
                <!--googleon: snippet-->
            </div>
        </div>

<!--Start talks here-->
        <div class="basic-section" id="sec_talks">
            <h1 class="title">Presentations</h1>
            <div class="talk" data-year="2019" data-journal="TEDx">
                <h2>Automatically Refocusing Reading Glasses</h2>
                <p><a href="https://www.youtube.com/watch?v=iAknMPnn_V8">TEDx Beacon Street, November 2019</a></p>
                <p><a href="https://www.ted.com/talks/nitish_padmanaban_autofocusing_reading_glasses_of_the_future">Autofocusing reading glasses of the future, (posted on TED)</a></p>
            </div>
            <div class="talk" data-year="2019" data-journal="SIGGRAPH (Talks), OSA (Frontiers in Optics)">
                <h2>Autofocals: Evaluating Gaze-Contingent Eyeglasses for Presbyopes</h2>
                <p>ACM SIGGRAPH Talks, August 2019</p>
                <p>OSA Frontiers in Optics, September 2019</p>
            </div>
            <div class="talk" data-year="2019" data-journal="Samsung Forum">
                <h2>Autofocal Correction for Presbyopes and Its Application to AR/VR</h2>
                <p><a href="https://www.youtube.com/watch?v=0zdF7CeFDAE">Samsung Forum, June 2019</a></p>
            </div>
            <div class="talk" data-year="2019" data-journal="SID">
                <h2>Fundamentals of Virtual- and Augmented-Reality Technologies</h2>
                <p>SID Display Week Short Courses, May 2019</p>
            </div>
            <div class="talk" data-year="2018, 2019" data-journal="Electronic Imaging">
                <h2>Build Your Own VR Display: An Introduction to VR Display Systems for Hobbyists and Educators</h2>
                <p>Electronic Imaging Short Courses, January 2018</p>
                <p>Electronic Imaging Short Courses, January 2019</p>
            </div>
            <div class="talk" data-year="2017" data-journal="SIGGRAPH (Talks)">
                <h2>Optimizing VR for All Users Through Adaptive Focus Displays</h2>
                <p>ACM SIGGRAPH Talks, July 2017</p>
            </div>
            <div class="talk" data-year="2017" data-journal="SIGGRAPH (Courses)">
                <h2>Build Your Own VR System: An Introduction to VR Displays and Cameras for Hobbyists and Educators</h2>
                <p>ACM SIGGRAPH Courses, July 2017</p>
            </div>
            <div class="talk" data-year="2017" data-journal="SID">
                <h2>Gaze-Contingent Adaptive Focus Near-Eye Displays</h2>
                <p>SID Display Week Invited Talk, May 2017</p>
            </div>
            <div class="talk" data-year="2017" data-journal="NVIDIA GTC">
                <h2>Computational Focus Tunable Near-Eye Displays</h2>
                <p>NVIDIA GPU Technology Conference, May 2017</p>
            </div>
            <div class="talk" data-year="2016" data-journal="Stanford mediaX">
                <h2>Panel: Frontiers in Technology</h2>
                <p>Stanford mediaX &ndash; Sensing and Tracking for 3D Narratives, October 2016</p>
            </div>
        </div>
    </div>

    <script type="text/javascript" src="scripts.js"></script>
</body>
</html>